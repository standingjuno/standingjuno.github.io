---
layout: post
title: "[Paper review] CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction"
tags: [Paper]
math: true
---

# CANVAS Paper Review
<https://worv-ai.github.io/canvas/>

---
CANVAS는 [마음에이아이(Maum.ai)](https://maum.ai/)에서 나온 Navigation 논문입니다. 2025년 AI Robotics KR에서 [디지스트 김기섭 교수님](https://sites.google.com/view/aprl-dgist)께서 소개해주시면서 알게 된 논문이고, VLA Navigation에 관심이 많아지면서 이번 기회를 통해 리뷰를 해보려고 합니다.

---
## Introduction
실제 로봇 Navigation 시나리오는 단순히 목적지에 도달하는 것을 넘어 복잡한 문제를 해결하는 것을 목표로 합니다. 예를 들어, 배달 로봇의 경우 차선을 준수하고 횡단보도를 이용해야 합니다.
따라서 로봇은 시나리오의 특정 요구 사항에 대응하면서 자신의 움직임을 최적화해야 합니다.

인간은 일반적으로 이러한 시나리오를 구두 명령, 원하는 경로 스케치 또는 이 둘의 조합과 같은 지시를 통해 전달합니다. 그러나 이러한 목표는 전반적인 목표이지만 구체성이 부족한 경우가 많습니다. 이러한 추상적이고 부정확한 지시를 실행 가능한 Navigation 계획으로 변환하려면 로봇은 상식(Commonsense)이 필요합니다.

로보틱스에서 상식은 인간이 의사 결정을 내릴 때 자연스럽게 사용하는 일반적인 이해를 의미하며, 로봇은 이 지식을 활용하여 경로를 유연하게 조정해야 하고, 환경과 사용자의 실제 의도에 의해 제기되는 상식 제약 조건을 준수함으로써 자신의 결정이 인간의 기대와 일치하도록 보장해야 합니다.

이러한 추상적인 인간 지시를 로봇 내비게이션에 통합하기 위한 새로운 프레임워크인 **CANVAS(Commonsense-Aware NavigAtion System)**을 소개합니다.

해당 논문의 기여는 세 가지입니다.
1. 인간이 멀티모달 입력을 사용하여 로봇과 쉽게 소통할 수 있게 해주는 새로운 프레임워크인 CANVAS를 소개하며, 인간의 지시가 모호하거나 노이즈가 있더라도 로봇이 효과적으로 내비게이션 목표를 달성하도록 보장합니다.
2. 48시간의 주행 데이터, 219km 규모의 상식 인지 내비게이션 로봇 훈련을 위한 데이터셋인 COMMAND를 소개합니다.
3. CANVAS가 성공률, 충돌률, 궤적 편차 거리 및 지시 위반율에서 ROS NavStack보다 우수함을 보여주는 광범위한 실험을 제시합니다.

---
## Dataset & Task
상호작용하는 로봇 내비게이션 프레임워크는 두 가지 주요 목표를 충족해야 합니다. 첫번째는 인간은 원하는 경로와 요구사항을 직관적으로 전달할 수 있어야 합니다. 두번째는 로봇은 이러한 지시를 정확하게 해석하고 실행할 수 있어야 합니다.

그러나 인간에게 소통을 단순화하는 것은 로봇에게는 종종 복잡하게 만드는데, 이는 인간이 자연스럽게 상대방이 자신의 상식 지식을 공유한다고 가정하기 때문입니다. 이러한 상식 지침이 불완전하거나 부정확할 때도 인간이 의미를 추론할 수 있게 하지만 로봇은 명시적이고 정확한 입력 없이는 어려움을 겪게 합니다.

이러한 문제를 해결하기 위해 **COMMAND**를 소개합니다. COMMAND는 로봇이 상식적 이해를 사용하여 노이즈가 많거나 추상적인 인간의 지침을 가장 원하는 궤적으로 변환할 수 있는지 평가하도록 설계된 포괄적인 데이터입니다.
COMMAND는 **NVIDIA Isaac Sim**을 사용하여 사무실, 거리, 과수원이라는 세 가지 고유한 환경에서 수집되었습니다. 이 데이터는 전문가가 작성한 고품질 스케치 지침 라벨과 원격 조작 데이터로 구성됩니다.

본 섹션에서는 데이터셋 구축 과정과 그에 따른 태스크 정의를 설명합니다.

#### Dataset
![image](/assets/images/canvas/canvas_image2.png)

COMMAND 데이터셋은 캔버스 맵, 스케치 및 언어 지침, 상식 제약 조건, 원격조종 기록을 포함합니다.

- 캔버스 맵은 점유 정보를 제공하며, 인간-로봇 간의 통신 인터페이스 역할입니다.
- S: 로봇이 따라야 할 맵 상의 손으로 그린 궤적(trajectory)으로 구성된 스케치 지침입니다. S를 수집할 때, 노이즈가 포함된 스케치 지침을 모델이 더 견고하게 처리할 수 있도록 훈련 데이터를 수집하기 위해 Precise 및 Misleading 조건을 모두 도입합니다.
- L: 목표와 관련 요구 사항을 설명하는 언어 지침입니다.
- 탐색 환경 E와 언어 지침 L에서 파생된 상식 제약 조건 C 세트를 정의합니다. 이러한 제약 조건은 로봇이 횡단보도 이용과 같은 적절한 탐색 행동을 보이는지 평가하는 데 도움이 됩니다.
- 인간 행동과 비교하여 로봇 행동을 최적화하고 평가하기 위해 원격조종 기록을 포함합니다. 이때 전면 카메라 이미지, 캔버스 맵 이미지, 인간이 시연한 궤적을 기록합니다.

#### Task Definition
- $X_f(t)$: 전방 시야 이미지.
- $H(t)$: 타임스텝 t-1까지의 로봇의 후향 궤적. H(t)는 로봇의 과거 위치를 기록하기 위해 오도메트리를 통해 추적됩니다.
- $X_c(t)$: 스케치 지침 S와 후향 궤적 $H(t)$를 동일한 지도에 결합하여 캔버스 지도 이미지 $X_c(t)$를 생성합니다.
- 각 단계마다, 로봇 $R$은 웨이포인트 시퀀스인 행동 $y(t) = [w_0, w_1, w_2, w_3]$을 생성합니다. 이 행동은 전방 시야 이미지 $X_f(t)$, 캔버스 지도 이미지 $X_c(t)$ 그리고 언어 지침 $L$에 조건화됩니다.
따라서 로봇의 행동은 다음과 같이 정의됩니다.

$$y(t) = R(X_f(t), X_c(t), L)$$ 

각 반복의 끝에서, 후향 궤적은 $p(t)$를 추가함으로써 업데이트됩니다. $p(t)$는 예측된 웨이포인트 $y(t)$를 통해 업데이트된 로봇의 위치를 나타내며, $H(t+1) = (p(1), p(2), ..., p(t))$를 결과로 합니다.
이 업데이트는 다음 캔버스 지도 이미지 $X_c(t+1)$에 반영되며, 전방 시야 이미지 $X_f(t+1)$도 로봇의 새로운 위치에 기반하여 업데이트됩니다. 이 과정은 로봇이 목적지에 도달하거나 최대 타임스텝 t=T에 도달할 때까지 계속됩니다.

---
## Method
본 절에서는 추상적인 인간의 지시와 구체적인 로봇 행동 간의 격차를 해소하도록 설계된 내비게이션 시스템인 CANVAS를 소개합니다. 이는 사전 훈련된 Vision-Language-Models(VLMs)의 상식적인 이해를 활용합니다.

#### Architecture
![image](/assets/images/canvas/canvas_image3.png)

전면 이미지 $X_f(t)$와 캔버스 지도 이미지 $X_c(t)$는 비전 인코더 $g_\phi$를 통해 처리됩니다. 이 결과로 두 개의 시각적 특성 $Z_f = g_\phi(X_f(t))$와 $Z_c = g_\phi(X_c(t))$가 생성됩니다. 프로젝터 $p_\phi$는 이러한 시각적 특징을 단어 임베딩 공간으로 투영하는 데 사용되며, 시각 토큰 시퀀스 $\tau_v = p_\phi(Z_f, Z_c)$를 생성합니다.

언어 토큰 시퀀스 $\tau_l$ 또한 언어 지침 $L$에서 가져옵니다. 시각 토큰 $\tau_v$와 언어 토큰 $\tau_l$을 대규모 언어 모델 $f_\phi(\cdot)$로 전달되며, 이는 waypoint 토큰 $[w_0, w_1, w_2, w_3] = f_\phi(\tau_v, \tau_l)$을 출력합니다. 또한 연속적인 waypoint를 토큰으로 이산화하기 위해 가장 간단한 방법인 K-means 클러스터링을 적용합니다.

#### Training
CANVAS는 waypoint 토큰들의 시퀀스로서 행동을 생성하도록 설계되었습니다. 로봇의 궤적은 N개의 연속된 waypoint로 표현될 수 있습니다. 훈련을 위한 Loss function은 다음과 같이 설정했습니다.

$$\mathcal{J}(\pi_\theta) = \sum_{n=1}^{N} \sum_{t=0}^{3} \log \pi_\theta(w_{t}^n \mid X_f(t)^n, X_c(t)^n, L^n)$$

모델은 현재 상태와 주어진 지침을 기반으로 다음 waypoint를 예측하는 분류 문제로 내비게이션을 재구성합니다.

#### Inference
추론 과정에서는 모델이 생성한 이산적인 웨이포인트 토큰이 연속적인 웨이포인트로 변환됩니다. 이 연속적인 웨이포인트들은 로봇의 액추에이터에 선형 및 각속도를 생성하기 위해 PD제어기에 입력됩니다.

---
## Experiments
![image](/assets/images/canvas/canvas_image4.png)

성능 평가를 위해 네 가지 주요 지표를 사용했습니다: Success Rate(SR), Collision Rate(CR), Trajectory Deviation Distance(TDD), Instruction Violation Rate(IVR).

비교 대상으로는 ROS NavStack을 사용했습니다. ROS NavStack은 직관적이지만 효과적인 규칙 기반 내비게이션 시스템으로 CANVAS와 비교됩니다. 이 시스템의 경우 스케치 지침을 단계별, 지점별 입력으로 변환했지만, 언어 지침은 수용할 수 없었습니다. NavStack을 사용한 모든 실험에서 동일한 하이퍼파라미터를 사용했습니다.

CANVAS는 두 가지 변형을 평가합니다. CANVAS-S는 비전 인코더 SigLIP-L, 텍스트 인코더 Qwen2-0.5B를 사용하여 모델 크기를 8B에서 0.7B로 줄여 실제 배포에 더 적합하게 만들었습니다. CANVAS-L은 사전 훈련된 가중치를 유지한 원래 비전 인코더인 Idefics2 아키텍처를 사용합니다.

두 모델 모두 LoRA를 사용하여 $\gamma=256$, $\alpha=512$, dropout=0.1로 훈련되었습니다. AdamW optimizer를 사용했으며, LLM의 경우 학습률이 $2e-5$, 프로젝터와 비전 인코더의 경우 $5e-5$로 다르게 설정되었고, 배치 크기는 32, 훈련 에포크는 5회였습니다. 각 모델은 128개의 웨이포인트 토큰을 사용합니다. 

CANVAS는 NVIDIA H100 GPU를 사용하여 추론되었습니다.

![image](/assets/images/canvas/canvas_image5.png)

CANVAS는 사무실 및 거리에서 NavStack과 유사한 SR, CR을 달성하며, 이는 CANVAS가 인간 시연으로부터 필수적인 네비게이션 동작을 효과적으로 학습할 수 있음을 시사합니다. 또한 거리, 과수원과 같은 환경에서는 CANVAS는 NavStack을 크게 능가합니다. CANVAS의 성공 요인을 분석하기 위해 정성적 분석을 수행한 결과, 과수원의 경우 56%는 바위에 걸려 발생하는 실패입니다. 과수원은 울퉁불퉁한 지형을 가지고 있으며 NavStack은 제한된 인식 능력으로 인해 바위와 풀과 같은 통행 가능한 영역을 구별하지 못하기 때문에 작지만 위험한 장애물(바위 등)을 피하는 데 어려움을 겪습니다. CANVAS는 시각적 입력을 활용하여 예상치 못한 장애물을 안정적으로 감지하고 시연을 통해 학습한 경험을 기반으로 탐색 위험을 평가합니다.

![image](/assets/images/canvas/canvas_image6.png)

거리(도로)에서는 차선 준수가 상식 제약 조건에 포함되며, 거리(보도)에서는 올바르게 도로를 횡단하고 보도 위에 머무르는 것이 포함됩니다. NavStack의 규칙 기반 접근 방식과 달리, CANVAS는 인간 시연으로부터 상식적 주행 규칙을 학습하며, 일관되게 더 낮은 IVR을 달성합니다.

![image](/assets/images/canvas/canvas_image7.png)

CANVAS의 시뮬레이션에서의 효과적인 내비게이션이 실제 환경으로 확장될 수 있음을 보여주는 것이 중요합니다. 실제 성능을 평가하기 위해 시뮬레이션의 기반으로 사용된 실제 사무실 환경에서 CANVAS를 테스트했습니다. 시뮬레이션 데이터만으로 훈련되었음에도 불구하고 CANVAS는 강력한 Sim2Real 전환 기능을 보여주며 실제 시나리오에서 안정적으로 작동했습니다.

마지막으로 CANVAS를 실시간 어플리케이션으로 배포하는 것에 대한 타당성을 평가합니다. 로봇에 통합된 NVIDIA AGX Orin에서 실험했으며, CANVAS-S는 400ms, CANVAS-L은 800ms를 기록합니다. 이는 상당한 지연 없이 효율적인 실제 세계 내비게이션 작업에 대한 잠재력을 강조합니다.

---
## Conclusion
모방 학습을 통해 인간의 시연으로부터 학습하는 새로운 상식 인식 내비게이션 시스템인 CANVAS를 제시합니다. CANVAS는 모호한 인간의 안내와 구체적인 로봇 행동 사이의 간극을 메우기 위해 상식 추론을 활용하는 동시에, 추상적인 스케치와 자연어를 사용하여 직관적인 인간 명령을 허용합니다.

모방 학습을 위한 COMMAND 데이터셋과 사전 훈련된 비전-언어 모델을 통해, CANVAS는 로봇이 암묵적인 인간 의도를 이해하고 인간의 기대에 부합하는 결정을 내릴 수 있도록 합니다. 실험 결과, ROS NavStack을 능가하는 성능을 보여주었습니다. 또한 CANVAS는 보지 못했던 환경과 실제 환경 모두에서 강력한 성능을 보여주며 일반화 능력을 강조했습니다.