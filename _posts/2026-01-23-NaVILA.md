---
layout: post
title: "[Paper review] NaVILA: Legged Robot Vision-Language-Action Model for Navigation"
tags: [Paper]
math: true
---

# NaVILA Paper Review

---
<https://navila-bot.github.io>

해당 논문은 4족보행 로봇이나 휴머노이드와 같은 다리가 달린 로봇의 내비게이션을 위해 작성된 논문입니다. 개인적으로 생각하는 VLA Navigation의 이상적인 모습이라고 생각하며, 해당 논문을 읽고 VLA Navigation을 연구해보고 싶다는 생각이 들었습니다.

---
## Introduction

현대 로보틱스에서 VLN(Vision-Language-Navigation)을 수행하는 능력은 핵심이 되었습니다. VLN을 통해 제공된 맵 없이 로봇은 언어 지침을 따라 보지 못한 환경을 탐색 가능하도록 예상됩니다. 본 논문에서는 다족 로봇으로 VLN을 확장합니다. 바퀴 대신 다리를 사용하면 로봇이 더 까다롭고 복잡한 시나리오를 탐색할 수 있습니다. 좁은 통로가 있는 지저분한 실험실 공간을 탐색하고, 집 안의 방과 방 사이를 이동하며, 작은 바위, 구멍, 도랑이 있는 울퉁불퉁한 지형과 같은 실외의 까다로운 환경을 처리할 수 있습니다.

본 논문에서는 공간적 위치 및 거리 추론을 위한 VLM의 최근 발전에 영감을 받아, 다족 로봇 VLN을 위한 2단계 프레임워크인 NaVILA를 제안합니다. VLM은 "30도 오른쪽으로 회전"과 같은 언어 형태의 중간 수준 행동(mid-level action)을 출력하도록 파인튜닝되고, low-level visual locomotion policy은 이 지시를 따르도록 학습됩니다.

해당 프레임워크의 장점은 세 가지입니다.
1. low-level 실행을 VLA와 분리함으로써, 동일한 VLA를 저수준 정책을 교체하여 서로 다른 로봇에 적용할 수 있습니다.
2. 행동을 중간 수준 언어 지침으로 표현하는 것은 실제 인간 동영상 및 질의 응답 작업(reasoning QS tasks)을 포함한 다양한 데이터 소스로 VLA 훈련을 가능하게 합니다.
3. NaVILA는 두 가지 분리된 timescale에서 작동합니다. VLA는 낮은 빈도로 실행되어 고수준 내비게이션 명령을 제공하며, 주행 정책은 실시간으로 작동합니다.

VLA를 훈련하기 위해
1. 기존 VLM 프레임워크 내에서 VLN의 과거 맥락과 현재 관찰을 통합하고,
2. VLN 작업을 위해 맞춤화된 특수 내비게이션 프롬프트를 생성하며,
3. 연속적인 환경에서의 내비게이션 개선을 위해 Youtube 비디오의 실제 데이터를 활용하고,
4. VLN 일반화 성능 향상을 위해 신중하게 큐레이션된 데이터셋 블렌드를 도입하는 방법을 시연합니다.

이러한 전략을 통해 우리는 범용 이미지 기반 VLM을 내비게이션 중심 에이전트로 미세 조정하는 동시에 일반 Vision-Language 데이터셋으로도 훈련하여 광범위한 일반화 능력을 유지할 수 있습니다.

강건한 locomotion 능력을 수행하기 위해, 단일 단계 접근 방식으로 시각 기반 locomotion policy를 학습합니다. 원시 LiDAR 포인트클라우드로부터 height map을 구성하고, sim-to-real 간극을 해소하기 위해 무작위화를 도입합니다. 이 컨트롤러는 우리의 VLA 모델의 출력을 받아 명령 속도로 변환하고 조인트 위치를 제어하여 이 속도를 추적합니다. 이러한 E2E 접근 방식은 강건하고 안전한 시각 로코모션 능력을 훈련할 수 있게 하며, 실제의 도전적인 환경에서 배포를 용이하게 합니다.

본 논문은 기존 VLN 벤치마크에서 SOTA 성능을 훨씬 뛰어넘어 성공률 17% 이상의 향상을 보였습니다. 또한, 우리의 단일 단계 로코모션 정책은 이전 정책 증류(policy distillation) 기반 방법보다 상당한 차이로 우수합니다.

---
## Method

![image](/assets/images/navila/navila_image1.png)

NaVILA는 high-level visual language 이해와 low-level locomotion control을 통합합니다. VLM을 이용하여 단일 시점 이미지를 처리하고 자연어로 웨이포인트 지침을 생성하며, locomotion policy가 이를 실시간 로봇 제어를 위한 정밀한 관절 움직임으로 변환합니다. VLM의 추론 능력과 이동 정책의 실행 능력간의 시너지는 NaVILA가 다양한 환경에 일반화할 수 있도록 합니다.

![image](/assets/images/navila/navila_image2.png)

### A. Taming VLMs for Vision Language Navigation

![image](/assets/images/navila/navila_image3.png)

본 연구는 이해와 생성을 위한 효율적인 VLM 계열인 **VILA**를 기반으로 접근 방식을 구축했습니다. VILA의 사전 훈련은 특히 다중 이미지 추론에 효과적인 것으로 입증되었으며, 이는 순차적인 이미지 관계 이해가 중요한 VLN 작업에 특히 적합합니다.

VILA는 비전 인코더, 프로젝터, LLM의 세 가지 주요 구성 요소로 이루어집니다. 비전 인코더는 입력 이미지를 처리하여 시각적 토큰 시퀀스로 변환합니다. 이러한 토큰은 다운샘플링된 후 MLP 프로젝터를 통해 언어 도메인으로 매핑됩니다. 이후 프로젝션된 토큰과 텍스트 토큰이 LLM으로 전송되어 자동 회귀 생성을 수행합니다.

비전-언어 내비게이션 작업에서 서로 다른 시간 단계의 이미지는 두 가지 distinct한 목적을 수행합니다. 시간 단계 t의 이미지는 현재 관찰을 나타내며, 이는 VLN 에이전트가 즉각적인 결정을 내리는 데 중요합니다(교차로에서 우회전하거나 목표에 도달했을 때 정지). 다른 한편으로, 시간 단계 t 이전의 프레임은 과거 프레임으로, 메모리 뱅크 역할을 하여 에이전트가 전반적인 진행 상황을 추적하는 데 도움을 줍니다(시작 위치를 기억하거나, 이미 방문한 장소를 추론하고 다음 단계를 계획). VILA에서 수행하는 것처럼 일정한 간격으로 프레임을 균일하게 샘플링하는 것은 이 두 가지 유형의 표현을 구분하지 못하기 때문에 이상적이지 않습니다. 따라서 현재 관찰로서 가장 최근 프레임 t를 추출하고, 첫 번째 프레임이 항상 포함되도록 이전 t-1 프레임에서 균일하게 프레임을 샘플링합니다. 또한 현재 및 과거 관찰은 다른 역할을 수행하므로, 메모리 프레임을 위한 video of historical observation과 최신 프레임을 위한 current observation과 같은 텍스트 단서를 사용하여 작업 프롬프트를 구분합니다. LLM의 학습 과정을 복잡하게 만들 수 있는 추가적인 특수 토큰을 도입하지는 않습니다. 대신, LLM의 추론 능력을 최대한 활용하기 위해 LLM의 입력과 출력을 모두 언어 도메인에 유지한다는 설계 원칙을 고수합니다.

최근 다른 연구에서는 인간 동영상에서 trajectory-instruction 쌍을 수집하는 것이 navigation 능력을 향상시킬 수 있음을 보여주었습니다. 하지만 이전 연구는 이산적인 navigation 환경으로 제한되었으며, navigation 모델을 직접 훈련하기보다는 도메인 간격을 줄이거나 랜드마크 이해를 개선하기 위해 주로 실제 동영상을 사전 학습에 사용했습니다. 본 논문은 Youtube 2K개 egocentric touring 비디오로 시작합니다. 이 비디오를 **entropy-based sampling**을 이용해 20K의 다양하고 대표적인 궤적으로 처리합니다. 다음으로 **MASt3R**을 사용하여 카메라 포즈를 추정하여 단계별 액션을 추출하고, VLM 기반 캡셔닝과 LLM 리패러징을 통해 각 궤적에 대한 자연어 지침을 생성합니다. 이 접근 방식은 연속 탐색을 위해 인간의 시연을 활용할 수 있게 해주며, 이는 이전에 달성하기 쉽지 않았던 기능입니다.

Supervised Fine-tuning(이하 SFT) 데이터는 강건한 VLA 모델을 개발하는 데 중요합니다. 모델은 특정 액션에 과적합되는 것을 피하면서 구체적인 작업에 특화되어야 합니다. 또한 실제 시나리오에 잘 일반화되어야 하며 광범위한 세계 지식을 유지해야 합니다. 본 논문의 SFT 데이터 혼합은 실제 비디오의 내비게이션 데이터, 시뮬레이션의 내비게이션 데이터, 보조 내비게이션 데이터, 일반 VQA 데이터셋 4가지 관점으로 설계되었습니다.

시뮬레이션 내비게이션 데이터의 경우, 연속 환경에서 사용 가능한 VLN 데이터셋은 제한적이며, R2R-CE 및 RxR-CE 만이 이산 VLN 버전에서 변환된 희소 경로 지점을 제공합니다. Habitat 시뮬레이터 내에서 두 데이터셋을 모두 활용하여 측지선 최단 경로를 따라 액션 시퀀스를 생성하기 위해 최단 경로 팔로워를 사용합니다. LLM이 거리와 각도에 대한 연속 값 레이블을 생성하도록 장려하기 위해, 본 논문은 최대 3개의 연속 액션을 병합합니다.

장면 이해도를 더욱 향상시키고 R2R-CE 및 RxR-CE의 제한된 지침을 해결하기 위해, 보조 내비게이션 데이터셋을 통합합니다. EnvDrop에서 증감된 지침을 사용하고 내비게이션 궤적 요약이라는 보조 작업을 도입합니다. 궤적 비디오가 주어지면, 첫 번째 프레임을 유지하고 기록 프레임을 균일하게 선택하여 프레임을 샘플링하며, 주석이 달린 지침을 레이블로 사용합니다. 그런 다음 LLM은 이러한 프레임을 기반으로 로봇의 궤적을 설명하는 작업을 수행합니다. 공간 장면 이해도를 더욱 향상시키기 위해, 사람 편집 질문과 3D 객체에 기반한 자유 형식 답변을 포함하는 3D 스캔 QA 쌍인 ScanQA 데이터셋을 통합합니다. 훈련을 위해, 이 작업을 지원하기 위해 원시 스캔의 RGB 이미지를 사용합니다.

마지막으로 모델의 일반 기능을 유지하기 위해, 일반 VQA 데이터셋을 통합합니다. 이러한 포괄적인 데이터셋 설계는 NaVILA가 새로운 장면과 실제 환경에 효과적으로 일반화될 수 있도록 보장합니다.

학습 과정은 시각 언어 코퍼스 사전 학습을 이미 완료한 stage-two VILA 모델로 시작합니다. 이후 표준 절차에 따라 SFT 데이터 블렌드를 적용하여 전체 VLM을 one epoch 동안 학습시킵니다. 이 학습 과정 동안, vision encoder, connector, LLM을 포함한 세 구성 요소 모두가 unfrozen 상태입니다.

추론 단계에서는 LLM 출력에서 행동 유형과 해당 인수들을 추출하기 위해 정규 표현식 파서(regular expression parser)를 구현합니다. 이 방법은 시뮬레이션 환경과 실제 실험 모두에서 효과성을 입증했으며, 모든 실험에서 모든 행동이 성공적으로 일치하고 매핑되었음을 경험적으로 확인했습니다.

### Visual Locomotion Policy

![image](/assets/images/navila/navila_image4.png)

VLM의 고수준 언어 내비게이션 명령을 해석하여 정밀한 관절 움직임으로 변환하는 E2E 시각 기반 제어 정책의 개발을 설명합니다. 해당 정책은 Isaac Lab을 사용하여 Isaac Sim 시뮬레이터에서 훈련된 후 실제 로봇에 직접 배포됩니다.

VLM은 "move forward", "turn left", "stop"와 같은 고정된 실행 가능 단어를 출력합니다. 이러한 명령을 속도 $0.5 \text{ m s}^{-1}, \frac{\pi}{6} \text{ rad s}^{-1}, 0$으로 변환하고, 특정 VLM 값에 맞추기 위해 해당 시간 동안 실행합니다.

제어 정책의 action space은 원하는 관절 위치 $q^d \in \mathbb{R}^{12}$로 정의되며, 이는 강성(stiffness)과 감쇠(dampness)를 사용하여 시뮬레이터의 토크 입력으로 변환됩니다. 우리는 policy 훈련을 위해 PPO 알고리즘을 채택했습니다. 훈련 중에는 critic이 특권적인 환경을 관측하고 value function를 생성하여 actor를 업데이트하며, 액터는 실제 세계에서 사용 가능한 센서 데이터만 받습니다. 크리틱의 관측 공간 $o^c$는 현재 시간 단계 t에서의 고유 감각(proprioception) 및 속도 명령, 그리고 로봇 주변의 특권적인 지형 높이 스캔을 포함합니다. 고유 감각 데이터에는 로봇의 선형 및 각속도, 방향, 관절 위치, 관절 속도, 이전 동작이 포함됩니다. actor의 관측 공간 $o^a$에서는 실제 세계에서 사용할 수 없는 선형 속도가 제외되며, 대신 고유 감각 데이터의 이력을 사용하여 이 정보를 암묵적으로 추론합니다. 로봇은 LiDAR 센서로부터의 height map을 통해 주변 지형을 인식합니다.

2.5D height map을 위해 Unitree L1 라이다를 사용했으며, supplementary에 나열된 매개변수에 따라 2.5D 맵을 생성합니다. 각 복셀 그리드에 대해 해당 범위 내의 최솟값을 선택하고, 최근 5개의 LiDAR 포인트 클라우드에 최댓값 필터를 적용하여 height map을 부드럽게 만듭니다.

기존의 teacher-student 훈련 패러다임을 활용하는 것과 달리, locomotion policy를 훈련하기 위해 single-stage 방식을 채택했습니다. single-stage 훈련은 policy distillation이 필요 없으므로 시간 효율성이 더 높습니다. 또한 policy는 환경과 직접 상호작용하여 새로운 전략을 탐색하고 발견할 가능성을 높입니다. Isaac Lab의 ray-casting 지원 덕분에 vision based RL policy training은 RTX4090 GPU에서 60K FPS 이상의 높은 처리량을 달성합니다.

---
## Conclusion & Limitations

본 논문의 NaVILA는 generic navigation을 위해 VLA와 locomotion skills를 통합하는 two-level framework입니다. high-level language commands를 생성하며, real-time locomotion policy는 obstacle avoidance를 처리하여 로봇 전반의 robustness를 향상시킵니다. 이 설계는 reasoning을 보존하고, overfitting을 방지하며, 더 나은 generalization을 위해 human videos로부터 직접 학습할 수 있도록 합니다. NaVILA는 classic VLN 벤치마크에서 17%의 성능 향상을 달성하고 distillation-based low-level policies를 능가하며, 다양한 환경과 legged robots에서 강력한 real-world performance를 보여줍니다.

한계점으로는 강력한 퍼포먼스를 보여주지만, 일부 real-world 케이스에서는 실패합니다. Enhancing generalizability와 spatial understanding을 위해 realistic simulation에서 더 큰 규모의 training을 수행하는 것이 도움이 될 수 있습니다. 또한, image-based VLM은 computationally intensive합니다. Long-context LLMs의 발전은 더 효율적인 sequence processing을 가능하게 하여 이를 완화할 수 있습니다.

